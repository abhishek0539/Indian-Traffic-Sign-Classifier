{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPY9TBdB0wHpN6BOeb6knxE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishek0539/Traffic_Sign_Prediction/blob/main/Traffic_Sign_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ejl8wiDEzPlq"
      },
      "outputs": [],
      "source": [
        "# Install and set up Kaggle API to download the dataset\n",
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and unzip the Indian Traffic Sign Dataset\n",
        "!kaggle datasets download -d neelpratiksha/indian-traffic-sign-dataset\n",
        "!unzip indian-traffic-sign-dataset.zip -d /content/indian-traffic-sign-dataset"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WCmssTHVzhKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from skimage import io, transform, exposure\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from collections import namedtuple\n",
        "\n",
        "# Directory paths\n",
        "csv_path = \"/content/indian-traffic-sign-dataset/Indian-Traffic Sign-Dataset/traffic_sign.csv\"\n",
        "image_folder = \"/content/indian-traffic-sign-dataset/Indian-Traffic Sign-Dataset/Images\"\n",
        "\n",
        "# Load dataset from CSV and images from folder\n",
        "def load_dataset(csv_path, image_folder):\n",
        "    data = pd.read_csv(csv_path)\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        class_id = row['ClassId']\n",
        "        class_folder = os.path.join(image_folder, str(class_id))\n",
        "\n",
        "        if not os.path.exists(class_folder):\n",
        "            print(f\"Warning: Folder {class_folder} does not exist. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        for image_name in os.listdir(class_folder):\n",
        "            img_path = os.path.join(class_folder, image_name)\n",
        "            if not os.path.isfile(img_path):\n",
        "                print(f\"Warning: File {img_path} does not exist. Skipping...\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                image = io.imread(img_path, as_gray=True)\n",
        "                X.append(image)\n",
        "                y.append(class_id)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img_path}: {e}\")\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_dataset(X, y):\n",
        "    X = np.array([transform.resize(image, (32, 32)) for image in X])\n",
        "    X = np.array([exposure.equalize_adapthist(image, clip_limit=0.1) for image in X])\n",
        "    X = np.expand_dims(X, axis=-1)\n",
        "    y = np.eye(59)[y]\n",
        "    X, y = shuffle(X, y)\n",
        "    return X.astype(np.float32), y.astype(np.float32)\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "X, y = load_dataset(csv_path, image_folder)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "X_train, y_train = preprocess_dataset(X_train, y_train)\n",
        "X_valid, y_valid = preprocess_dataset(X_valid, y_valid)\n",
        "X_test, y_test = preprocess_dataset(X_test, y_test)\n",
        "\n",
        "# Model parameters\n",
        "Parameters = namedtuple('Parameters', [\n",
        "    'num_classes', 'image_size', 'batch_size', 'max_epochs', 'log_epoch',\n",
        "    'print_epoch', 'learning_rate_decay', 'learning_rate', 'l2_reg_enabled',\n",
        "    'l2_lambda', 'early_stopping_enabled', 'early_stopping_patience',\n",
        "    'resume_training', 'conv1_k', 'conv1_d', 'conv1_p', 'conv2_k',\n",
        "    'conv2_d', 'conv2_p', 'conv3_k', 'conv3_d', 'conv3_p', 'fc4_size', 'fc4_p'\n",
        "])\n",
        "\n",
        "params = Parameters(\n",
        "    num_classes=59, image_size=32, batch_size=128, max_epochs=20, log_epoch=1,\n",
        "    print_epoch=1, learning_rate_decay=0.95, learning_rate=0.001, l2_reg_enabled=True,\n",
        "    l2_lambda=0.0001, early_stopping_enabled=True, early_stopping_patience=5,\n",
        "    resume_training=False, conv1_k=5, conv1_d=32, conv1_p=0.5, conv2_k=5,\n",
        "    conv2_d=64, conv2_p=0.5, conv3_k=5, conv3_d=128, conv3_p=0.5,\n",
        "    fc4_size=512, fc4_p=0.5\n",
        ")\n",
        "\n",
        "# TensorFlow model functions\n",
        "def conv_relu(input, kernel_size, depth):\n",
        "    conv = tf.keras.layers.Conv2D(depth, kernel_size, padding='same', activation='relu')(input)\n",
        "    return conv\n",
        "\n",
        "def pool(input, size):\n",
        "    return tf.keras.layers.MaxPooling2D(pool_size=size, strides=size, padding='same')(input)\n",
        "\n",
        "def model_pass(input, params):\n",
        "    conv1 = conv_relu(input, kernel_size=params.conv1_k, depth=params.conv1_d)\n",
        "    pool1 = pool(conv1, size=2)\n",
        "    pool1 = tf.keras.layers.Dropout(params.conv1_p)(pool1)\n",
        "\n",
        "    conv2 = conv_relu(pool1, kernel_size=params.conv2_k, depth=params.conv2_d)\n",
        "    pool2 = pool(conv2, size=2)\n",
        "    pool2 = tf.keras.layers.Dropout(params.conv2_p)(pool2)\n",
        "\n",
        "    conv3 = conv_relu(pool2, kernel_size=params.conv3_k, depth=params.conv3_d)\n",
        "    pool3 = pool(conv3, size=2)\n",
        "    pool3 = tf.keras.layers.Dropout(params.conv3_p)(pool3)\n",
        "\n",
        "    flat = tf.keras.layers.Flatten()(pool3)\n",
        "    fc4 = tf.keras.layers.Dense(params.fc4_size, activation='relu')(flat)\n",
        "    fc4 = tf.keras.layers.Dropout(params.fc4_p)(fc4)\n",
        "\n",
        "    logits = tf.keras.layers.Dense(params.num_classes)(fc4)\n",
        "    return logits\n",
        "\n",
        "# Build TensorFlow model\n",
        "inputs = tf.keras.Input(shape=(params.image_size, params.image_size, 1))\n",
        "labels = tf.keras.Input(shape=(params.num_classes,))\n",
        "\n",
        "logits = model_pass(inputs, params)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=params.learning_rate),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=params.early_stopping_patience,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(\n",
        "    x=X_train,\n",
        "    y=y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    epochs=params.max_epochs,\n",
        "    batch_size=params.batch_size,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate on validation and test sets\n",
        "valid_loss, valid_accuracy = model.evaluate(X_valid, y_valid)\n",
        "print(\"Validation Accuracy:\", valid_accuracy)\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "MSFH1zoy0CKU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drawing the graph for Training Accuracy and Training Loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract training and validation metrics from the history object\n",
        "epochs = range(1, len(history.history['accuracy']) + 1)\n",
        "\n",
        "# Plot accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, history.history['loss'], label='Training Loss')\n",
        "plt.plot(epochs, history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Fbur5jl4_IiG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from skimage import transform, exposure\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import os\n",
        "\n",
        "# Go to Directory\n",
        "current_directory = os.getcwd()\n",
        "\n",
        "# Load the sign mapping CSV file from the current directory\n",
        "sign_mapping_path = os.path.join(current_directory, '/content/indian-traffic-sign-dataset/Indian-Traffic Sign-Dataset/traffic_sign.csv')\n",
        "sign_mapping_df = pd.read_csv(sign_mapping_path)\n",
        "\n",
        "# Create a dictionary mapping class IDs to sign names\n",
        "sign_mapping = dict(zip(sign_mapping_df['ClassId'], sign_mapping_df['Name']))\n",
        "\n",
        "# Define the upload widget\n",
        "upload = widgets.FileUpload(accept='image/*', multiple=False)\n",
        "\n",
        "# Preprocess the uploaded image for prediction.\n",
        "def preprocess_image(image):\n",
        "    if image.mode != 'L':\n",
        "        image = image.convert('L')\n",
        "    image = np.array(image)\n",
        "\n",
        "    # Preprocess the image\n",
        "    image = transform.resize(image, (32, 32))\n",
        "    image = exposure.equalize_adapthist(image, clip_limit=0.1)\n",
        "    image = np.expand_dims(image, axis=-1)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    return image\n",
        "\n",
        "# Predict the class of the image using the loaded model and return the sign name.\n",
        "def predict_image_class(image):\n",
        "    image = preprocess_image(image)\n",
        "    logits = model.predict(image)\n",
        "    predicted_class_index = np.argmax(logits, axis=-1)\n",
        "    sign_name = sign_mapping.get(predicted_class_index[0], \"Unknown Sign\")\n",
        "    return sign_name\n",
        "\n",
        "# Handle the upload event and display results.\n",
        "def on_upload_change(change):\n",
        "    uploaded_file = change['new']\n",
        "\n",
        "    if uploaded_file:\n",
        "        file_content = next(iter(uploaded_file.values()))['content']\n",
        "        try:\n",
        "            image = Image.open(io.BytesIO(file_content))\n",
        "            print(f\"Image format: {image.format}, size: {image.size}, mode: {image.mode}\")\n",
        "\n",
        "            predicted_sign_name = predict_image_class(image)\n",
        "\n",
        "            plt.figure(figsize=(6, 6))\n",
        "            plt.imshow(image, cmap='gray')\n",
        "            plt.title(f\"Predicted Sign: {predicted_sign_name}\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "upload.observe(on_upload_change, names='value')\n",
        "\n",
        "display(upload)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BlTlZBIGB4IW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}